{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Como o UniVAD Funciona: Uma Explicação Detalhada\n",
        "\n",
        "Pense no UniVAD como um **detetive especialista** que nunca viu a \"cena do crime\" (a sua caixa de papelão) antes. Para descobrir se algo está errado em uma nova foto, ele só precisa de algumas fotos de referência de como a cena \"normalmente\" se parece.\n",
        "\n",
        "Ele não passa por um \"treinamento\" formal, mas usa sua vasta experiência (modelos pré-treinados) para fazer comparações inteligentes. O trabalho desse detetive é dividido em três módulos principais, que são como três especialistas diferentes em sua equipe:\n",
        "\n",
        "#### **Módulo 1: O Analista de Cena (Contextual Component Clustering - C³)**\n",
        "\n",
        "Este é o primeiro especialista a agir. Seu trabalho é **identificar e isolar todos os objetos importantes** na imagem.\n",
        "*   **Ferramentas:** Ele usa dois modelos poderosos:\n",
        "    1.  **GroundingDINO:** Pense nele como o especialista em \"o quê\". Você dá a ele uma descrição em texto (ex: `\"cardboard box . packaging\"`) e ele encontra onde esses objetos estão na imagem, desenhando \"caixas\" (bounding boxes) ao redor deles.\n",
        "    2.  **Segment Anything Model (SAM):** Este é o especialista em \"onde exatamente\". Ele pega as caixas desenhadas pelo GroundingDINO e cria uma máscara de segmentação pixel a pixel, um contorno perfeito para cada objeto detectado.\n",
        "*   **Resultado:** No final, este módulo entrega um conjunto de \"peças de quebra-cabeça\" (máscaras), onde cada peça é um componente da imagem (idealmente, a caixa inteira).\n",
        "\n",
        "#### **Módulo 2: O Perito Forense (Component-Aware Patch Matching - CAPM)**\n",
        "\n",
        "Este especialista procura por **anomalias estruturais e de textura** — defeitos físicos como arranhões, manchas, impressão errada, amassados, etc.\n",
        "*   **Como Funciona:** Ele usa a técnica de \"correspondência de patches\":\n",
        "    1.  Pega cada \"peça do quebra-cabeça\" (componente/máscara) da imagem de teste.\n",
        "    2.  Divide essa peça em milhares de pequenos \"quadrados\" (patches).\n",
        "    3.  Para cada patch, ele o compara com uma biblioteca gigante de patches \"normais\" que ele extraiu das imagens de referência.\n",
        "*   **Lógica da Anomalia:** Se um patch da imagem de teste for muito diferente de **todos** os patches normais que ele já viu, ele é considerado suspeito. A pontuação de anomalia do patch é a sua \"estranheza\" em relação à normalidade.\n",
        "*   **Ferramentas:** Para fazer essa comparação, ele não compara os pixels diretamente. Ele usa \"olhos\" sofisticados para extrair a essência de cada patch:\n",
        "    *   **CLIP (ViT-L):** Excelente em entender o **conteúdo semântico** (o \"significado\" do que está no patch).\n",
        "    *   **DINOv2 (ViT-G/L):** Excelente em entender a **estrutura fina, textura e geometria** do patch.\n",
        "*   **Resultado:** Um \"mapa de calor estrutural\" que destaca as áreas da imagem com defeitos físicos.\n",
        "\n",
        "#### **Módulo 3: O Detetive Lógico (Graph-Enhanced Component Modeling - GECM)**\n",
        "\n",
        "Este especialista procura por **anomalias lógicas** — erros de \"regras\" ou de montagem, como uma peça faltando, uma peça extra, ou uma peça no lugar errado.\n",
        "*   **Como Funciona:** Ele não olha para os pequenos patches, mas para as \"peças do quebra-cabeça\" inteiras.\n",
        "    1.  Ele analisa as características de cada peça: sua área, cor média, posição na imagem e sua feature de \"deep learning\" (extraída pelo CLIP/DINOv2).\n",
        "    2.  Ele modela as relações entre as peças em um \"grafo\". Por exemplo: \"a peça A (logo) está sempre em cima da peça B (aba da caixa)\".\n",
        "*   **Lógica da Anomalia:** Ele compara o conjunto de peças e suas relações na imagem de teste com o que ele aprendeu das imagens de referência. Se a imagem de teste tem uma peça a mais, ou se a relação entre as peças está incorreta, ele atribui um score de anomalia lógica.\n",
        "*   **Resultado:** Um \"mapa de calor lógico\" que destaca componentes que estão ausentes, em excesso ou mal posicionados.\n",
        "\n",
        "#### **O Veredito Final (Agregação)**\n",
        "\n",
        "No final, o UniVAD combina os mapas de calor do Perito Forense (CAPM) e do Detetive Lógico (GECM) para criar um **mapa de anomalia final** e um **score de anomalia geral**, que nos diz o quão \"anormal\" a imagem é como um todo."
      ],
      "metadata": {
        "id": "4mxFYYhGnzK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 2. Resumo do Trabalho Realizado até Agora\n",
        "\n",
        "Esta é uma lista detalhada e organizada de todo o progresso feito, que é bastante significativo.\n",
        "\n",
        "**I. Coleta e Preparação do Dataset**\n",
        "*   **Criação de um Dataset Customizado:** Foram capturadas 554 imagens de alta variedade, cobrindo 43 caixas de papelão distintas (13 normais, 30 anômalas).\n",
        "*   **Variedade de Condições:** As imagens foram tiradas em múltiplos ambientes (chão e esteira), com múltiplos ângulos (3-4 por caixa) e utilizando duas câmeras de celular diferentes, garantindo robustez e realismo.\n",
        "*   **Coleta de Vídeos:** Foram gravados vídeos do processo real em três máquinas distintas (cortadora, impressora, coladeira) para futura análise ou expansão do dataset.\n",
        "*   **Organização Estruturada:** O dataset foi organizado na estrutura de pastas `train/good`, `test/good` e `test/bad`, compatível com os frameworks de detecção de anomalia.\n",
        "\n",
        "**II. Configuração do Ambiente e Otimização de Performance**\n",
        "*   **Resolução de Conflitos de Versão:** Foram instaladas versões específicas das bibliotecas `transformers` e `tokenizers` para garantir a compatibilidade com o ambiente de execução (Python 3.12).\n",
        "*   **Gerenciamento de Memória (VRAM):** O tamanho da imagem de entrada foi padronizado em `224x224` para permitir a execução dos modelos em GPUs com memória limitada.\n",
        "*   **Redução de Modelos:** Para otimizar o uso de memória, foram testadas versões menores de modelos-chave:\n",
        "    *   O **Segment Anything Model** foi trocado da versão `SAM-H` (Huge) para a `SAM-B` (Base).\n",
        "    *   O **DINOv2** foi trocado da versão `ViT-G` (Giant) para a `ViT-L` (Large).\n",
        "*   **Carregamento Eficiente de Modelos:** O gargalo de performance de ~30s por imagem foi identificado e resolvido. A solução foi refatorar o `component_segmentation.py` para uma classe (`SegmentationHandler`) que carrega os modelos pesados (GroundingDINO, SAM) na VRAM uma única vez no início, em vez de a cada imagem.\n",
        "\n",
        "**III. Refatoração e Aprimoramento do Pipeline de Software**\n",
        "*   **Unificação do Fluxo de Trabalho:** O processo foi consolidado em um único script (`test_univad.py`), que realiza segmentação e inferência em sequência para cada imagem. Isso simplificou drasticamente o processo de execução e depuração.\n",
        "*   **Implementação de Parâmetros de Teste:** O `test_univad.py` foi aprimorado com parâmetros de linha de comando para permitir experimentação flexível:\n",
        "    *   `--debug`: Ativa o salvamento de mapas de calor e scores intermediários.\n",
        "    *   `--filter_with_mask`: Habilita a filtragem do score e do heatmap final usando a máscara de segmentação, focando a análise apenas no objeto de interesse.\n",
        "    *   `--top_k_percent`: Substitui o cálculo de score baseado no `max()` por uma média dos `k%` pixels mais anômalos, tornando a pontuação mais robusta.\n",
        "\n",
        "**IV. Otimização da Segmentação e Detecção**\n",
        "*   **Engenharia de Prompts:** Foram realizados testes e melhorias nos prompts de texto (`text_prompt` e `background_prompt`) no arquivo de configuração do GroundingDINO para melhorar a precisão da segmentação, ensinando o modelo a focar na caixa e ignorar a esteira e o fundo.\n",
        "\n",
        "---\n",
        "No modo MULTI, que é o único que utiliza o DinoFeaturizer, tinha que carregar esse modelo no init independente dele ser usado apenas no MULTI. Agora, tanto from modules import DinoFeaturizer quanto self.dino_net = DinoFeaturizer() são realizados apenas no caso de ser MULTI.\n",
        "\n",
        "Limpeza da RAM logo após carregar cada modelo na GPU"
      ],
      "metadata": {
        "id": "jK90Yxikn8Py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "t0Ue9tD8qFyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "3KvV3g9JqFtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Dúvidas:\n",
        "\n",
        "- Misturar as imagens das duas câmeras ou usar apenas de uma delas para isolar efeitos que poderiam ter de usar uma imagem de referência da Câmera 1 para avaliar imagens da Câmera 2.\n",
        "\n",
        "- Usar sub-conjunto do dataset para os testes?\n",
        "\n",
        "- Usar cross-validation nos testes?\n",
        "\n",
        "- Os testes propostos fazem sentido? São suficientes?"
      ],
      "metadata": {
        "id": "AAVzSOSjofed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "4CYrzefpqJN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "FMxU0kSlqJEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Plano de Ação (TO-DO) Detalhado\n",
        "\n",
        "**FASE 1: Definição da Configuração Base (Performance vs. Acurácia)**\n",
        "\n",
        "*O objetivo desta fase é encontrar o melhor trade-off entre velocidade e acurácia, escolhendo a combinação de modelos que será usada em todos os testes seguintes.*\n",
        "\n",
        "*   **[ ] TO-DO 1.1: Avaliação do Prompt de Segmentação**.\n",
        "    *   **Contexto:** O primeiro passo é garantir que a segmentação seja a melhor possível.\n",
        "    *   **Execução:**\n",
        "        1.  Execute o script com o **Prompt A** (ex: `\"cardboard box\"`).\n",
        "        2.  Execute o script com o **Prompt B** (ex: `\"cardboard box . packaging\"`).\n",
        "        3.  Execute o script com o **Prompt C** (ex: `\"the cardboard box on the conveyor\"`).\n",
        "    *   **Análise:** Inspecione visualmente as máscaras de segmentação geradas (arquivos `grounding_mask_color.png`). Conte o número de \"falhas graves\" (ex: caixa não detectada, detecção grosseiramente errada) para cada prompt.\n",
        "    *   **Resultado:** Escolha o prompt que produzir o menor número de segmentações ruidosas. **Este prompt será usado para todos os testes subsequentes.**\n",
        "\n",
        "*   **[ ] TO-DO 1.2: Teste de Tempo de Inferência vs. Acurácia**.\n",
        "    *   **Contexto:** Usando o melhor prompt da etapa anterior, vamos testar diferentes \"motores\" para o UniVAD.\n",
        "    *   **Execução:** Execute o teste completo para cada uma das seguintes configurações de modelo:\n",
        "        1.  **Config A (Leve):** SAM-B + DINOv2-L (Large)\n",
        "        2.  **Config B (Pesada):** SAM-B + DINOv2-G (Giant)\n",
        "        3.  **Config C (Qualidade Máxima / Original):** SAM-H (Huge) + DINOv2-G (Giant)\n",
        "    *   **Métricas:** Para cada configuração, registre:\n",
        "        *   **Tempo Médio por Imagem (s)**.\n",
        "        *   **Acurácia Geral Final (%)**.\n",
        "    *   **Resultado:** Crie uma tabela comparativa. A decisão aqui será crucial: o ganho (se houver) de acurácia da Config B ou C justifica o tempo extra de processamento em relação à Config A? A configuração escolhida aqui será a **configuração base** para todas as fases seguintes.\n",
        "\n",
        "**FASE 2: Otimização de Hiperparâmetros de Software**\n",
        "\n",
        "*O objetivo é ajustar os parâmetros do algoritmo usando a \"configuração base\" definida na Fase 1.*\n",
        "\n",
        "*   **[ ] TO-DO 2.1: Testar o Impacto do `--top_k_percent`**.\n",
        "    *   **Execução:** Usando a configuração base, rode o teste com diferentes valores: `0.0` (equivalente a Max), `0.01` (1%), `0.05` (5%) e `1.0` (equivalente a Mean).\n",
        "    *   **Métrica:** Comparar a **Acurácia Geral (%)** para cada valor.\n",
        "    *   **Análise:** Determine qual método de agregação de score (Max, Top 1%, Top 5% ou Mean) produz a melhor acurácia. O valor vencedor será usado nos testes de robustez.\n",
        "\n",
        "*   **[ ] TO-DO 2.2: Testar o Impacto do `--filter_with_mask`**.\n",
        "    *   **Execução:** Usando a configuração base e o melhor `top_k_percent` encontrado, execute o teste duas vezes:\n",
        "        1.  **Sem** a flag `--filter_with_mask`.\n",
        "        2.  **Com** a flag `--filter_with_mask`.\n",
        "    *   **Métrica:** Comparar a **Acurácia Geral (%)** entre as duas execuções.\n",
        "    *   **Pergunta:** Focar a análise de score apenas na área da caixa segmentada melhora a performance da classificação?\n",
        "\n",
        "**FASE 3: Testes de Robustez e Sensibilidade do Modelo**\n",
        "\n",
        "*O objetivo é estressar o modelo (com a melhor configuração encontrada até agora) para entender seus limites e capacidades.*\n",
        "\n",
        "*   **[ ] TO-DO 3.1: Teste de `k-shot` (Número de Imagens de Referência)**.\n",
        "    *   **Execução:**\n",
        "        1.  Rode com `--k_shot 1`.\n",
        "        2.  Rode com `--k_shot 4`, usando 4 imagens de referência da mesma caixa \"boa\", mas de ângulos diferentes.\n",
        "    *   **Pergunta:** Usar mais exemplos de referência de diferentes perspectivas torna o modelo mais robusto?\n",
        "\n",
        "*   **[ ] TO-DO 3.2: Teste de Contexto da Referência (Chão vs. Esteira)**.\n",
        "    *   **Execução:**\n",
        "        1.  Rode o teste usando uma imagem de referência de uma caixa no chão.\n",
        "        2.  Rode o teste usando uma imagem de referência de uma caixa na esteira.\n",
        "    *   **Pergunta:** O desempenho melhora quando o contexto da referência (esteira) é o mesmo do teste?\n",
        "\n",
        "*   **[ ] TO-DO 3.3: Teste de Qualidade da Imagem de Referência**.\n",
        "    *   **Contexto:** Vamos avaliar o impacto de uma referência \"ruim\" em um conjunto de teste \"ruim\".\n",
        "    *   **Execução:** Prepare um pequeno subconjunto de imagens de teste que estejam borradas.\n",
        "        1.  Execute o teste nesse subconjunto usando uma **referência normal (nítida)**.\n",
        "        2.  Execute o teste nesse subconjunto usando uma **referência borrada**.\n",
        "    *   **Pergunta:** Usar uma referência com o mesmo tipo de \"defeito\" (borrão) ajuda o modelo a ignorá-lo e focar em anomalias reais?\n",
        "\n",
        "*   **[ ] TO-DO 3.4: Teste de Generalização entre Caixas e Câmeras**.\n",
        "    *   **Execução:**\n",
        "        1.  **Variação de Caixa:** Execute o teste usando a Caixa A (boa) como referência. Anote a acurácia. Depois, execute novamente usando a Caixa B (boa, mas de outro modelo) como referência. Anote a acurácia.\n",
        "        2.  **Variação de Câmera (Cross-Camera Test):**\n",
        "            *   Execute o teste usando uma imagem de referência da **Câmera 1** e avalie a acurácia **apenas** no subconjunto de imagens da **Câmera 2**.\n",
        "            *   Execute o teste usando uma imagem de referência da **Câmera 2** e avalie a acurácia **apenas** no subconjunto de imagens da **Câmera 1**.\n",
        "    *   **Pergunta:** O modelo é sensível a pequenas mudanças no modelo da caixa ou na câmera usada, ou ele consegue generalizar bem?\n",
        "\n",
        "*   **[ ] TO-DO 3.5: Teste de Detecção de Ângulo**.\n",
        "    *   **Análise:** Após rodar os testes, pegue uma caixa específica que tenha um defeito claro e que foi fotografada de múltiplos ângulos.\n",
        "    *   **Verificação:** Olhe os logs e os heatmaps salvos para essa caixa.\n",
        "    *   **Pergunta:** O modelo foi capaz de detectar o mesmo defeito consistentemente, independentemente do ângulo da foto?"
      ],
      "metadata": {
        "id": "k24oIqd8lY_T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqFRodBxk6Gm"
      },
      "source": [
        "### Links de Referência para o UniVAD\n",
        "* **Repositório no GitHub:** [https://github.com/FantasticGNU/UniVAD](https://github.com/FantasticGNU/UniVAD)\n",
        "* **Artigo (Paper):** [https://arxiv.org/pdf/2412.03342](https://arxiv.org/pdf/2412.03342)\n",
        "* **Site do Projeto:** [https://uni-vad.github.io/](https://uni-vad.github.io/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recurse-submodules https://github.com/MateusPereiraAlves/UniVAD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8h0s7p1LCFf",
        "outputId": "aacf55c0-21ef-41ad-c828-d8b70d920354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UniVAD'...\n",
            "remote: Enumerating objects: 536, done.\u001b[K\n",
            "remote: Counting objects: 100% (536/536), done.\u001b[K\n",
            "remote: Compressing objects: 100% (387/387), done.\u001b[K\n",
            "remote: Total 536 (delta 153), reused 501 (delta 130), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (536/536), 7.87 MiB | 29.94 MiB/s, done.\n",
            "Resolving deltas: 100% (153/153), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG5LVKd3yWzX",
        "outputId": "a64b7ed8-69e3-4a38-cbe3-4ee408460e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UniVAD\n"
          ]
        }
      ],
      "source": [
        "%cd UniVAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M2wf5mt_yNVh",
        "outputId": "e629fe0e-a030-47df-ee25-d50d9f015ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydensecrf@ git+https://github.com/lucasb-eyer/pydensecrf.git (from -r requirements.txt (line 18))\n",
            "  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-install-thdh31lu/pydensecrf_b35f2b13115a4e27887f806b713cf17b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lucasb-eyer/pydensecrf.git /tmp/pip-install-thdh31lu/pydensecrf_b35f2b13115a4e27887f806b713cf17b\n",
            "  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit 2723c7fa4f2ead16ae1ce3d8afe977724bb8f87f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ram@ git+https://github.com/xinyu1205/recognize-anything.git (from -r requirements.txt (line 37))\n",
            "  Cloning https://github.com/xinyu1205/recognize-anything.git to /tmp/pip-install-thdh31lu/ram_4573cacec1e54956bd45b3e284351ac8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/xinyu1205/recognize-anything.git /tmp/pip-install-thdh31lu/ram_4573cacec1e54956bd45b3e284351ac8\n",
            "  Resolved https://github.com/xinyu1205/recognize-anything.git to commit 7cb804a8609e9f4b1a50b7f31436d2df40bb9481\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting segment_anything@ git+https://github.com/facebookresearch/segment-anything.git (from -r requirements.txt (line 38))\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-install-thdh31lu/segment-anything_a32803179e5d4dfd9e3b5ef81b8ca917\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-install-thdh31lu/segment-anything_a32803179e5d4dfd9e3b5ef81b8ca917\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from -r requirements.txt (line 1))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting clip (from -r requirements.txt (line 2))\n",
            "  Downloading clip-0.2.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.8.1)\n",
            "Collecting fairscale (from -r requirements.txt (line 4))\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fiftyone (from -r requirements.txt (line 5))\n",
            "  Downloading fiftyone-1.10.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting ftfy (from -r requirements.txt (line 6))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting gradio==4.21.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-4.21.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.36.0)\n",
            "Collecting ipdb (from -r requirements.txt (line 9))\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting kornia (from -r requirements.txt (line 10))\n",
            "  Downloading kornia-0.8.1-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2.0.2)\n",
            "Requirement already satisfied: opencv_python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (4.12.0.88)\n",
            "Requirement already satisfied: opencv_python_headless in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (4.12.0.88)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (11.3.0)\n",
            "Collecting pycocoevalcap (from -r requirements.txt (line 16))\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (2.0.10)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (6.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (2024.11.6)\n",
            "Requirement already satisfied: Requests in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 21)) (2.32.4)\n",
            "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 22)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (1.16.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 24)) (75.2.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (0.25.2)\n",
            "Collecting supervision (from -r requirements.txt (line 26))\n",
            "  Downloading supervision-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (3.2.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (1.0.21)\n",
            "Collecting torch==2.2.0 (from -r requirements.txt (line 29))\n",
            "  Downloading torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchmetrics (from -r requirements.txt (line 30))\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 31)) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 32)) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 33)) (4.57.1)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 34)) (0.20.0)\n",
            "Collecting wget (from -r requirements.txt (line 35))\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting yapf (from -r requirements.txt (line 36))\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting segment-anything-hq (from -r requirements.txt (line 39))\n",
            "  Downloading segment_anything_hq-0.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting prefetch_generator (from -r requirements.txt (line 40))\n",
            "  Downloading prefetch_generator-1.0.3.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.21.0->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (0.120.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (0.6.4)\n",
            "Collecting gradio-client==0.12.0 (from gradio==4.21.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.12.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio==4.21.0->-r requirements.txt (line 7))\n",
            "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting numpy (from -r requirements.txt (line 12))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (2.2.2)\n",
            "Collecting Pillow (from -r requirements.txt (line 15))\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (0.14.2)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (2.10.0)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.21.0->-r requirements.txt (line 7))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.21.0->-r requirements.txt (line 7)) (0.38.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 29)) (3.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 29)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 29)) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 29)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.0->-r requirements.txt (line 29))\n",
            "  Downloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.12.0->gradio==4.21.0->-r requirements.txt (line 7))\n",
            "  Downloading websockets-11.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->-r requirements.txt (line 29)) (12.6.85)\n",
            "Collecting argcomplete (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting async_lru>=2 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from fiftyone->-r requirements.txt (line 5)) (4.13.5)\n",
            "Collecting boto3 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading boto3-1.40.65-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from fiftyone->-r requirements.txt (line 5)) (5.5.2)\n",
            "Collecting dacite<2,>=1.6.0 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading dacite-1.9.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from fiftyone->-r requirements.txt (line 5)) (0.3.8)\n",
            "Collecting Deprecated (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from fiftyone->-r requirements.txt (line 5)) (4.14.0)\n",
            "Collecting hypercorn>=0.13.2 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading hypercorn-0.17.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.12/dist-packages (from fiftyone->-r requirements.txt (line 5)) (1.33)\n",
            "Collecting mongoengine~=0.29.1 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading mongoengine-0.29.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting motor~=3.6.0 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading motor-3.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting plotly>=6.1.1 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading plotly-6.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pprintpp (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from fiftyone->-r requirements.txt (line 5)) (5.9.5)\n",
            "Collecting pymongo~=4.9.2 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading pymongo-4.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from fiftyone->-r requirements.txt (line 5)) (2025.2)\n",
            "Collecting retrying (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting rtree (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting sseclient-py<2,>=1.7.2 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting sse-starlette<1,>=0.10.3 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading sse_starlette-0.10.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: starlette>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from fiftyone->-r requirements.txt (line 5)) (0.49.1)\n",
            "Collecting strawberry-graphql>=0.262.4 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading strawberry_graphql-0.284.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fiftyone->-r requirements.txt (line 5)) (0.9.0)\n",
            "Collecting xmltodict (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading xmltodict-1.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting universal-analytics-python3<2,>=1.0.1 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting pydash (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading pydash-8.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting fiftyone-brain<0.22,>=0.21.3 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading fiftyone_brain-0.21.4-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting fiftyone-db<2.0,>=0.4 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading fiftyone_db-1.3.0.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting voxel51-eta<0.16,>=0.15.1 (from fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading voxel51_eta-0.15.1-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->-r requirements.txt (line 6)) (0.2.14)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->-r requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.12/dist-packages (from ipdb->-r requirements.txt (line 9)) (7.34.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipdb->-r requirements.txt (line 9)) (4.4.2)\n",
            "Collecting kornia_rs>=0.1.9 (from kornia->-r requirements.txt (line 10))\n",
            "  Downloading kornia_rs-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 11)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 11)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 11)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 11)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 11)) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv_python (from -r requirements.txt (line 13))\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv_python_headless (from -r requirements.txt (line 14))\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from Requests->-r requirements.txt (line 21)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from Requests->-r requirements.txt (line 21)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from Requests->-r requirements.txt (line 21)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from Requests->-r requirements.txt (line 21)) (2025.10.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn->-r requirements.txt (line 22)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn->-r requirements.txt (line 22)) (3.6.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r requirements.txt (line 25)) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r requirements.txt (line 25)) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->-r requirements.txt (line 25)) (0.4)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from supervision->-r requirements.txt (line 26)) (0.7.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->-r requirements.txt (line 28)) (0.6.2)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->-r requirements.txt (line 30))\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from -r requirements.txt (line 31))\n",
            "  Downloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.20.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "INFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "  Downloading torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "  Downloading torchvision-0.18.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchvision-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading torchvision-0.17.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchvision-0.17.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 33)) (0.22.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer->-r requirements.txt (line 34)) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->-r requirements.txt (line 34)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer->-r requirements.txt (line 34)) (13.9.4)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf->-r requirements.txt (line 36)) (4.5.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.21.0->-r requirements.txt (line 7)) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.21.0->-r requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.21.0->-r requirements.txt (line 7)) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.21.0->-r requirements.txt (line 7)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.21.0->-r requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: h2>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from hypercorn>=0.13.2->fiftyone->-r requirements.txt (line 5)) (4.3.0)\n",
            "Collecting priority (from hypercorn>=0.13.2->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading priority-2.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wsproto>=0.14.0 (from hypercorn>=0.13.2->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=7.31.1->ipdb->-r requirements.txt (line 9))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 9)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 9)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 9)) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 9)) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 9)) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 9)) (4.9.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==4.21.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.21.0->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.21.0->-r requirements.txt (line 7)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.21.0->-r requirements.txt (line 7)) (0.4.2)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo~=4.9.2->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 11)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->-r requirements.txt (line 34)) (4.0.0)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql>=0.262.4->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting lia-web>=0.2.1 (from strawberry-graphql>=0.262.4->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading lia_web-0.2.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "\u001b[33mWARNING: typer 0.20.0 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: glob2 in /usr/local/lib/python3.12/dist-packages (from voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5)) (0.7)\n",
            "Collecting jsonlines (from voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting paramiko<4,>=3 (from voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting py7zr (from voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting rarfile (from voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5)) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.12/dist-packages (from voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5)) (5.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->fiftyone->-r requirements.txt (line 5)) (2.8)\n",
            "Collecting botocore<1.41.0,>=1.40.65 (from boto3->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading botocore-1.40.65-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from Deprecated->fiftyone->-r requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==4.21.0->-r requirements.txt (line 7)) (0.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch->fiftyone->-r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0->-r requirements.txt (line 29)) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.24.1->gradio==4.21.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone->-r requirements.txt (line 5)) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone->-r requirements.txt (line 5)) (4.1.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb->-r requirements.txt (line 9)) (0.8.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.21.0->-r requirements.txt (line 7)) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.21.0->-r requirements.txt (line 7)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.21.0->-r requirements.txt (line 7)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.21.0->-r requirements.txt (line 7)) (0.28.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->-r requirements.txt (line 34)) (0.1.2)\n",
            "Collecting bcrypt>=3.2 (from paramiko<4,>=3->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.12/dist-packages (from paramiko<4,>=3->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5)) (43.0.3)\n",
            "Collecting pynacl>=1.5 (from paramiko<4,>=3->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading pynacl-1.6.0-cp38-abi3-manylinux_2_34_x86_64.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb->-r requirements.txt (line 9)) (0.7.0)\n",
            "Collecting texttable (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pycryptodomex>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5)) (3.23.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5)) (1.1.0)\n",
            "Collecting pyzstd>=0.16.1 (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading pyzstd-0.18.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting pyppmd<1.3.0,>=1.1.0 (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading pyppmd-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading pybcj-1.0.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting multivolumefile>=0.2.3 (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5))\n",
            "  Downloading inflate64-1.0.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.3->paramiko<4,>=3->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko<4,>=3->voxel51-eta<0.16,>=0.15.1->fiftyone->-r requirements.txt (line 5)) (2.23)\n",
            "Downloading gradio-4.21.0-py3-none-any.whl (17.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl (755.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.4/755.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-0.12.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m998.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading fiftyone-1.10.0-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
            "Downloading kornia-0.8.1-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading supervision-0.26.1-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.0-cp312-cp312-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading segment_anything_hq-0.3-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading dacite-1.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading fiftyone_brain-0.21.4-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hypercorn-0.17.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading mongoengine-0.29.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading motor-3.6.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plotly-6.3.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
            "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading strawberry_graphql-0.284.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.7/308.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Downloading voxel51_eta-0.15.1-py2.py3-none-any.whl (934 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.65-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading pydash-8.0.5-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.1/102.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Downloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-1.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading botocore-1.40.65-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading lia_web-0.2.3-py3-none-any.whl (13 kB)\n",
            "Downloading paramiko-3.5.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
            "Downloading py7zr-1.0.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflate64-1.0.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.0/97.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Downloading pybcj-1.0.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynacl-1.6.0-cp38-abi3-manylinux_2_34_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppmd-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzstd-0.18.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (429 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m429.9/429.9 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: clip, fairscale, pydensecrf, wget, ram, segment_anything, prefetch_generator, fiftyone-db\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=6989 sha256=fd3fa9e7fafcdbddbbe3176733b4801eb8f14e43f8486560427815513fbc9c02\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/fd/54/9d4e15cf829b871199a7cd3597e869a514d1624a0a43076896\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332208 sha256=e75b2af63bb6a4abdfa8892a7f59695dc921a1b706f0937e7ef78d1ca8596381\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/88/aa/d84b2cf1bad6b273cbf661640141a82c7b9f496e024f80aac0\n",
            "  Building wheel for pydensecrf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydensecrf: filename=pydensecrf-1.0-cp312-cp312-linux_x86_64.whl size=3494585 sha256=e1f813b522f245549372e09a5eeef0a84df1d3ba458f79e3dde5a0be0508e396\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8um727ap/wheels/5f/63/9b/ad8357747651277615ec2094c768471e8ecde7d7b53564f24d\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=eabc8e8a745e56a627cbd546d0d2dbbc16e6754be3adadbf6a3d7abe6f7e28d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
            "  Building wheel for ram (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ram: filename=ram-0.0.1-py3-none-any.whl size=128837 sha256=68234c957cfa853bf858ea1570e756e0f092655b3b8b5bea86f14906694a4267\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8um727ap/wheels/fb/22/8f/65b4960e80f59a09828812e50c1112198fe699fa986af87522\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=b9b894a02fa94ba24de0cd4dcb241d210f28599c73a65e8407b8dadb78757ec6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8um727ap/wheels/29/82/ff/04e2be9805a1cb48bec0b85b5a6da6b63f647645750a0e42d4\n",
            "  Building wheel for prefetch_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prefetch_generator: filename=prefetch_generator-1.0.3-py3-none-any.whl size=4758 sha256=62ce4e06a5f333af88ba71e1d8da8d0b29e8abd45ca03fa135bf01a679010d57\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/88/c7/3b5afc342fc80a599ce41ba9000cf8a71261991c35cf088edf\n",
            "  Building wheel for fiftyone-db (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fiftyone-db: filename=fiftyone_db-1.3.0-py3-none-manylinux1_x86_64.whl size=42156245 sha256=b508d0c2c8e4b3306044a350c81fb9d649d235cc5c1bc415906ba050bc7ec3a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/77/bb/e3973ec8ec71b8f8b2bf18e9fd8ee6f7304df16833ad4704bd\n",
            "Successfully built clip fairscale pydensecrf wget ram segment_anything prefetch_generator fiftyone-db\n",
            "Installing collected packages: wget, texttable, sseclient-py, segment_anything, ram, pydensecrf, prefetch_generator, pprintpp, clip, addict, yapf, xmltodict, wsproto, websockets, triton, tomlkit, rtree, retrying, rarfile, pyzstd, pyppmd, pydash, pybcj, priority, plotly, Pillow, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, multivolumefile, markupsafe, lightning-utilities, lia-web, kornia_rs, jsonlines, jmespath, jedi, inflate64, graphql-core, ftfy, fiftyone-db, dnspython, Deprecated, dacite, bcrypt, async_lru, argcomplete, aiofiles, strawberry-graphql, pynacl, pymongo, py7zr, opencv_python_headless, opencv_python, nvidia-cusolver-cu12, nvidia-cudnn-cu12, hypercorn, botocore, universal-analytics-python3, torch, sse-starlette, s3transfer, pycocoevalcap, paramiko, motor, mongoengine, ipdb, gradio-client, voxel51-eta, torchvision, torchmetrics, supervision, kornia, fiftyone-brain, fairscale, boto3, segment-anything-hq, gradio, fiftyone\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.3\n",
            "    Uninstalling tomlkit-0.13.3:\n",
            "      Successfully uninstalled tomlkit-0.13.3\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.24.1\n",
            "    Uninstalling plotly-5.24.1:\n",
            "      Successfully uninstalled plotly-5.24.1\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: opencv_python_headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: opencv_python\n",
            "    Found existing installation: opencv-python 4.12.0.88\n",
            "    Uninstalling opencv-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-4.12.0.88\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: sse-starlette\n",
            "    Found existing installation: sse-starlette 3.0.2\n",
            "    Uninstalling sse-starlette-3.0.2:\n",
            "      Successfully uninstalled sse-starlette-3.0.2\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.13.3\n",
            "    Uninstalling gradio_client-1.13.3:\n",
            "      Successfully uninstalled gradio_client-1.13.3\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.49.1\n",
            "    Uninstalling gradio-5.49.1:\n",
            "      Successfully uninstalled gradio-5.49.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-genai 1.46.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "mcp 1.19.0 requires sse-starlette>=1.6.1, but you have sse-starlette 0.10.3 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.2.0 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-adk 1.17.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 11.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.3.1 Pillow-10.4.0 addict-2.4.0 aiofiles-23.2.1 argcomplete-3.6.3 async_lru-2.0.5 bcrypt-5.0.0 boto3-1.40.65 botocore-1.40.65 clip-0.2.0 dacite-1.9.2 dnspython-2.8.0 fairscale-0.4.13 fiftyone-1.10.0 fiftyone-brain-0.21.4 fiftyone-db-1.3.0 ftfy-6.3.1 gradio-4.21.0 gradio-client-0.12.0 graphql-core-3.2.7 hypercorn-0.17.3 inflate64-1.0.3 ipdb-0.13.13 jedi-0.19.2 jmespath-1.0.1 jsonlines-4.0.0 kornia-0.8.1 kornia_rs-0.1.9 lia-web-0.2.3 lightning-utilities-0.15.2 markupsafe-2.1.5 mongoengine-0.29.1 motor-3.6.1 multivolumefile-0.2.3 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 opencv_python-4.11.0.86 opencv_python_headless-4.11.0.86 paramiko-3.5.1 plotly-6.3.1 pprintpp-0.4.0 prefetch_generator-1.0.3 priority-2.0.0 py7zr-1.0.0 pybcj-1.0.6 pycocoevalcap-1.2 pydash-8.0.5 pydensecrf-1.0 pymongo-4.9.2 pynacl-1.6.0 pyppmd-1.2.0 pyzstd-0.18.0 ram-0.0.1 rarfile-4.2 retrying-1.4.2 rtree-1.4.1 s3transfer-0.14.0 segment-anything-hq-0.3 segment_anything-1.0 sse-starlette-0.10.3 sseclient-py-1.8.0 strawberry-graphql-0.284.1 supervision-0.26.1 texttable-1.7.0 tomlkit-0.12.0 torch-2.2.0 torchmetrics-1.8.2 torchvision-0.17.0 triton-2.2.0 universal-analytics-python3-1.1.1 voxel51-eta-0.15.1 websockets-11.0.3 wget-3.2 wsproto-1.2.0 xmltodict-1.0.2 yapf-0.43.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "475a7e4bd0d44ef6951d7d148133ab0e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H67k7kkyZhQ",
        "outputId": "1ac775ab-b6ad-4417-e1d8-1a7ce3492b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UniVAD/models/GroundingDINO\n",
            "Obtaining file:///content/UniVAD/models/GroundingDINO\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from groundingdino==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from groundingdino==0.1.0) (0.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from groundingdino==0.1.0) (4.57.1)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.12/dist-packages (from groundingdino==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.12/dist-packages (from groundingdino==0.1.0) (0.43.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from groundingdino==0.1.0) (1.0.21)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from groundingdino==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from groundingdino==0.1.0) (4.11.0.86)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.12/dist-packages (from groundingdino==0.1.0) (0.26.1)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (from groundingdino==0.1.0) (2.0.10)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from supervision->groundingdino==0.1.0) (1.16.3)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from supervision->groundingdino==0.1.0) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from supervision->groundingdino==0.1.0) (6.0.3)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from supervision->groundingdino==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.12/dist-packages (from supervision->groundingdino==0.1.0) (10.4.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from supervision->groundingdino==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.12/dist-packages (from supervision->groundingdino==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm->groundingdino==0.1.0) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->groundingdino==0.1.0) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.12/dist-packages (from torch->groundingdino==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->groundingdino==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->groundingdino==0.1.0) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->groundingdino==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->groundingdino==0.1.0) (0.22.1)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf->groundingdino==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->groundingdino==0.1.0) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision->groundingdino==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision->groundingdino==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision->groundingdino==0.1.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision->groundingdino==0.1.0) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision->groundingdino==0.1.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision->groundingdino==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision->groundingdino==0.1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision->groundingdino==0.1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision->groundingdino==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision->groundingdino==0.1.0) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->groundingdino==0.1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->groundingdino==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision->groundingdino==0.1.0) (1.17.0)\n",
            "Installing collected packages: groundingdino\n",
            "  Running setup.py develop for groundingdino\n",
            "Successfully installed groundingdino-0.1.0\n"
          ]
        }
      ],
      "source": [
        "%cd models/GroundingDINO\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/UniVAD/pretrained_ckpts\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -O ./sam_vit_b.pth\n",
        "!wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLyTIVnGhZSE",
        "outputId": "36c3c6d3-2ff1-4f94-9156-6f9704f7b6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UniVAD/pretrained_ckpts\n",
            "--2025-11-04 12:54:26--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.37.84, 13.35.37.111, 13.35.37.90, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.37.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375042383 (358M) [binary/octet-stream]\n",
            "Saving to: ‘./sam_vit_b.pth’\n",
            "\n",
            "./sam_vit_b.pth     100%[===================>] 357.67M   168MB/s    in 2.1s    \n",
            "\n",
            "2025-11-04 12:54:29 (168 MB/s) - ‘./sam_vit_b.pth’ saved [375042383/375042383]\n",
            "\n",
            "--2025-11-04 12:54:29--  https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/611591640/f221e500-c2fc-4fd3-b84e-8ad92a6923f3?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-04T13%3A46%3A13Z&rscd=attachment%3B+filename%3Dgroundingdino_swint_ogc.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-04T12%3A45%3A17Z&ske=2025-11-04T13%3A46%3A13Z&sks=b&skv=2018-11-09&sig=z3KlYiLRFoyV4lyy3ErA5JBrbYSj00X2SOWIR%2FRgdo4%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MjI2NDQ2OSwibmJmIjoxNzYyMjYwODY5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.FVuE0SLUyIvFlwPEGACBuKIfvWIa4nnJgfz52xYb-q4&response-content-disposition=attachment%3B%20filename%3Dgroundingdino_swint_ogc.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-11-04 12:54:29--  https://release-assets.githubusercontent.com/github-production-release-asset/611591640/f221e500-c2fc-4fd3-b84e-8ad92a6923f3?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-04T13%3A46%3A13Z&rscd=attachment%3B+filename%3Dgroundingdino_swint_ogc.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-04T12%3A45%3A17Z&ske=2025-11-04T13%3A46%3A13Z&sks=b&skv=2018-11-09&sig=z3KlYiLRFoyV4lyy3ErA5JBrbYSj00X2SOWIR%2FRgdo4%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MjI2NDQ2OSwibmJmIjoxNzYyMjYwODY5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.FVuE0SLUyIvFlwPEGACBuKIfvWIa4nnJgfz52xYb-q4&response-content-disposition=attachment%3B%20filename%3Dgroundingdino_swint_ogc.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 693997677 (662M) [application/octet-stream]\n",
            "Saving to: ‘groundingdino_swint_ogc.pth’\n",
            "\n",
            "groundingdino_swint 100%[===================>] 661.85M  55.4MB/s    in 9.2s    \n",
            "\n",
            "2025-11-04 12:54:38 (71.7 MB/s) - ‘groundingdino_swint_ogc.pth’ saved [693997677/693997677]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWoa61y9yiiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e5c9a2-03e9-4df5-c170-45938f9487b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UniVAD/pretrained_ckpts\n",
            "--2025-11-03 12:36:40--  https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_h.pth\n",
            "Resolving huggingface.co (huggingface.co)... 18.161.6.107, 18.161.6.46, 18.161.6.94, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.161.6.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6486dc523457cf1120c70b8b/3cf6fbc02437cad20d8f50b8623c6b942d7f6570f82db01027cc10734c4e25a8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251103T123640Z&X-Amz-Expires=3600&X-Amz-Signature=b34ee193110359f70f14ba71d220e7e4932fdb795c82e85f53b09f711f5977dd&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27sam_hq_vit_h.pth%3B+filename%3D%22sam_hq_vit_h.pth%22%3B&x-id=GetObject&Expires=1762177000&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MjE3NzAwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDg2ZGM1MjM0NTdjZjExMjBjNzBiOGIvM2NmNmZiYzAyNDM3Y2FkMjBkOGY1MGI4NjIzYzZiOTQyZDdmNjU3MGY4MmRiMDEwMjdjYzEwNzM0YzRlMjVhOCoifV19&Signature=WVXZ%7EcNyCmnQiNCLxm9yBkoVwMr1d28Gh8tjB-X6ti2hEhPvYRQabGN2DDy81lkJ2-LLCCkZxsxwatGqF991txbD385OaaRbTkMlW8EFo7IblQfW-c95XNyoK7YQFOT0Cm0OFFbdu0wqkRy9TXRN8et6v1RNcAjcGps5l%7EH5jrwj%7Ee9WT9uOKZDzZcJLEeX32VAWPYSXhjb3lDhkm56Y3aG64SEWua0SjcEYP2sAftli3Uinwf5hdIVK%7EdiLl7lo3gPgh8yAPyWvQztiYsPYp1RfxEH1GuHyM7-OHtn2kN4jO2kf1V4HlUOnEk0TuAQzTdThW6sJTcRbUu8PP8S5Mg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-11-03 12:36:40--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6486dc523457cf1120c70b8b/3cf6fbc02437cad20d8f50b8623c6b942d7f6570f82db01027cc10734c4e25a8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251103T123640Z&X-Amz-Expires=3600&X-Amz-Signature=b34ee193110359f70f14ba71d220e7e4932fdb795c82e85f53b09f711f5977dd&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27sam_hq_vit_h.pth%3B+filename%3D%22sam_hq_vit_h.pth%22%3B&x-id=GetObject&Expires=1762177000&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MjE3NzAwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDg2ZGM1MjM0NTdjZjExMjBjNzBiOGIvM2NmNmZiYzAyNDM3Y2FkMjBkOGY1MGI4NjIzYzZiOTQyZDdmNjU3MGY4MmRiMDEwMjdjYzEwNzM0YzRlMjVhOCoifV19&Signature=WVXZ%7EcNyCmnQiNCLxm9yBkoVwMr1d28Gh8tjB-X6ti2hEhPvYRQabGN2DDy81lkJ2-LLCCkZxsxwatGqF991txbD385OaaRbTkMlW8EFo7IblQfW-c95XNyoK7YQFOT0Cm0OFFbdu0wqkRy9TXRN8et6v1RNcAjcGps5l%7EH5jrwj%7Ee9WT9uOKZDzZcJLEeX32VAWPYSXhjb3lDhkm56Y3aG64SEWua0SjcEYP2sAftli3Uinwf5hdIVK%7EdiLl7lo3gPgh8yAPyWvQztiYsPYp1RfxEH1GuHyM7-OHtn2kN4jO2kf1V4HlUOnEk0TuAQzTdThW6sJTcRbUu8PP8S5Mg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.63, 18.238.217.126, 18.238.217.88, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2570940653 (2.4G)\n",
            "Saving to: ‘sam_hq_vit_h.pth’\n",
            "\n",
            "sam_hq_vit_h.pth    100%[===================>]   2.39G  95.8MB/s    in 21s     \n",
            "\n",
            "2025-11-03 12:37:01 (116 MB/s) - ‘sam_hq_vit_h.pth’ saved [2570940653/2570940653]\n",
            "\n",
            "--2025-11-03 12:37:01--  https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/611591640/f221e500-c2fc-4fd3-b84e-8ad92a6923f3?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-03T13%3A26%3A34Z&rscd=attachment%3B+filename%3Dgroundingdino_swint_ogc.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-03T12%3A25%3A51Z&ske=2025-11-03T13%3A26%3A34Z&sks=b&skv=2018-11-09&sig=S0CoDB08mspCFanpaRvP1EhMNW0q%2BFC25ZWNJEyQJ3Q%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MjE3NzAyMiwibmJmIjoxNzYyMTczNDIyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.HANhihgJhZJqDh02lHqK2kNphnIk4n1ncC8TMtlYRvE&response-content-disposition=attachment%3B%20filename%3Dgroundingdino_swint_ogc.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-11-03 12:37:02--  https://release-assets.githubusercontent.com/github-production-release-asset/611591640/f221e500-c2fc-4fd3-b84e-8ad92a6923f3?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-03T13%3A26%3A34Z&rscd=attachment%3B+filename%3Dgroundingdino_swint_ogc.pth&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-03T12%3A25%3A51Z&ske=2025-11-03T13%3A26%3A34Z&sks=b&skv=2018-11-09&sig=S0CoDB08mspCFanpaRvP1EhMNW0q%2BFC25ZWNJEyQJ3Q%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MjE3NzAyMiwibmJmIjoxNzYyMTczNDIyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.HANhihgJhZJqDh02lHqK2kNphnIk4n1ncC8TMtlYRvE&response-content-disposition=attachment%3B%20filename%3Dgroundingdino_swint_ogc.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 693997677 (662M) [application/octet-stream]\n",
            "Saving to: ‘groundingdino_swint_ogc.pth’\n",
            "\n",
            "groundingdino_swint 100%[===================>] 661.85M   262MB/s    in 2.5s    \n",
            "\n",
            "2025-11-03 12:37:04 (262 MB/s) - ‘groundingdino_swint_ogc.pth’ saved [693997677/693997677]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# %cd /content/UniVAD/pretrained_ckpts\n",
        "# !wget https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_h.pth\n",
        "# !wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdj628Bq3V_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06bbf405-bbb3-4958-92ce-458737c62414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UniVAD\n"
          ]
        }
      ],
      "source": [
        "%cd /content/UniVAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ory58_-F3hZn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "00fc0b59-e1e8-42b8-d929-9cf5c22d5726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.3 setuptools-80.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "15e79b6c7d6545fe9fa40c1c145a72f4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Para usar segment_components.py\n",
        "\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "\n",
        "# Forçar versão do transformers compatível\n",
        "!pip install transformers==4.44.2 --upgrade --quiet\n",
        "\n",
        "# Instala tokenizers que tem wheel para Python 3.12\n",
        "!pip install tokenizers==0.19.1 --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import re\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================================================================\n",
        "# ======================== PAINEL DE CONTROLE DO DATASET =======================\n",
        "# ==============================================================================\n",
        "# Altere apenas os parâmetros nesta seção para configurar seu dataset.\n",
        "\n",
        "# --- 1. Seleção da Câmera ---\n",
        "# Escolha de qual câmera processar as imagens.\n",
        "# Opções: 'cam1', 'cam2', 'both'\n",
        "SELECT_CAMERA = 'both'\n",
        "\n",
        "# --- 2. Definição das Amostras \"Boas\" (Normais) ---\n",
        "# Coloque aqui os NÚMEROS das caixas que são consideradas \"boas\".\n",
        "# Qualquer caixa com um número fora desta lista será considerada \"ruim\" (anômala).\n",
        "GOOD_BOX_NUMBERS = {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 32, 33}\n",
        "\n",
        "# --- 3. Seleção das Amostras de TREINO ---\n",
        "# Todas as imagens de treino devem pertencer às classes \"boas\".\n",
        "# O script irá verificar isso e emitir um aviso se você tentar usar uma amostra \"ruim\" para treino.\n",
        "\n",
        "# Abordagem A: Por NÚMERO da caixa.\n",
        "# Todas as imagens das caixas com estes números serão movidas para 'train/good'.\n",
        "# TRAIN_BOX_NUMBERS = {15, 19}\n",
        "TRAIN_BOX_NUMBERS = {}\n",
        "\n",
        "# Abordagem B: Por NOME DE ARQUIVO específico.\n",
        "# Útil para selecionar manualmente algumas imagens de caixas que também irão para teste.\n",
        "# INCLUA O PREFIXO DA CÂMERA (cam1_ ou cam2_) no nome do arquivo. ---\n",
        "# IMPORTANTE: Use o formato do nome com underscores '_', não pontos '.'.\n",
        "TRAIN_SPECIFIC_FILENAMES = {\n",
        "    'cam1_board15_1.jpg',\n",
        "    # Exemplo para Câmera 2: 'cam2_board19_2.jpg',\n",
        "    # Adicione outros nomes de arquivo específicos aqui se desejar.\n",
        "}\n",
        "\n",
        "# --- 4. Filtro de Localização da Câmera ---\n",
        "# Escolha quais tipos de imagem incluir no dataset.\n",
        "# Opções disponíveis: 'chao', 'esteira1', 'esteira2'.\n",
        "# Você pode incluir uma, duas ou todas as três.\n",
        "INCLUDE_LOCATIONS = {'chao', 'esteira1', 'esteira2'}\n",
        "\n",
        "# --- 5. Exclusão de Amostras ---\n",
        "# Coloque aqui os NÚMEROS das caixas que você quer IGNORAR completamente.\n",
        "# Elas não serão incluídas nem no treino, nem no teste.\n",
        "EXCLUDE_BOX_NUMBERS = {\n",
        "    9, 10, 11\n",
        "}\n",
        "\n",
        "# --- 6. Caminhos de Origem e Destino ---\n",
        "# Caminho da pasta no seu Google Drive com as imagens originais.\n",
        "# ESTA VARIÁVEL SERÁ AJUSTADA AUTOMATICAMENTE PELA SELEÇÃO DA CÂMERA.\n",
        "SOURCE_DIR_CAM1 = '/content/drive/MyDrive/TCC_Dataset/Câmera1'\n",
        "SOURCE_DIR_CAM2 = '/content/drive/MyDrive/TCC_Dataset/Câmera2'\n",
        "\n",
        "# Caminho base de destino no Colab para o dataset estruturado.\n",
        "DEST_BASE_DIR = '/content/UniVAD/data/CardboardBox'\n",
        "\n",
        "# ==============================================================================\n",
        "# ======================== FIM DO PAINEL DE CONTROLE ===========================\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# --- Lógica do Script (não precisa alterar daqui para baixo) ---\n",
        "\n",
        "def get_location(filename: str) -> str:\n",
        "    \"\"\"Identifica a localização da imagem com base no nome do arquivo.\"\"\"\n",
        "    fn_lower = filename.lower()\n",
        "    if '.b2.jpg' in fn_lower: # Mais robusto que endswith\n",
        "        return 'esteira2'\n",
        "    elif '.b.jpg' in fn_lower: # Mais robusto que endswith\n",
        "        return 'esteira1'\n",
        "    else:\n",
        "        return 'chao'\n",
        "\n",
        "print(\"Montando Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Lógica para selecionar o diretório de origem ---\n",
        "source_dirs_to_process = []\n",
        "if SELECT_CAMERA == 'cam1':\n",
        "    source_dirs_to_process.append((SOURCE_DIR_CAM1, 'cam1'))\n",
        "elif SELECT_CAMERA == 'cam2':\n",
        "    source_dirs_to_process.append((SOURCE_DIR_CAM2, 'cam2'))\n",
        "elif SELECT_CAMERA == 'both':\n",
        "    source_dirs_to_process.append((SOURCE_DIR_CAM1, 'cam1'))\n",
        "    source_dirs_to_process.append((SOURCE_DIR_CAM2, 'cam2'))\n",
        "else:\n",
        "    raise ValueError(\"SELECT_CAMERA deve ser 'cam1', 'cam2' ou 'both'.\")\n",
        "\n",
        "# --- Preparação de Pastas ---\n",
        "dest_class_dir = os.path.join(DEST_BASE_DIR, 'cardboard_box')\n",
        "train_good_dir = os.path.join(dest_class_dir, 'train', 'good')\n",
        "test_good_dir = os.path.join(dest_class_dir, 'test', 'good')\n",
        "test_bad_dir = os.path.join(dest_class_dir, 'test', 'bad')\n",
        "\n",
        "if os.path.exists(dest_class_dir):\n",
        "    print(f\"Limpando diretório de destino antigo: {dest_class_dir}\")\n",
        "    shutil.rmtree(dest_class_dir)\n",
        "\n",
        "print(\"Criando nova estrutura de pastas de destino...\")\n",
        "os.makedirs(train_good_dir, exist_ok=True)\n",
        "os.makedirs(test_good_dir, exist_ok=True)\n",
        "os.makedirs(test_bad_dir, exist_ok=True)\n",
        "\n",
        "# Regex para extrair o número principal da caixa (ex: 'board14' -> 14)\n",
        "filename_pattern = re.compile(r'board(\\d+)', re.IGNORECASE)\n",
        "\n",
        "# --- Contadores para o Relatório Final ---\n",
        "counters = {\n",
        "    'train_good': 0, 'test_good': 0, 'test_bad': 0,\n",
        "    'skipped_excluded': 0, 'skipped_location': 0,\n",
        "    'skipped_pattern': 0, 'skipped_other': 0,\n",
        "    'warn_bad_in_train': 0\n",
        "}\n",
        "total_files_in_sources = 0\n",
        "\n",
        "print(\"\\nIniciando a organização do dataset...\")\n",
        "for source_dir, camera_prefix in source_dirs_to_process:\n",
        "    print(f\"\\nProcessando diretório: {source_dir} (prefixo: '{camera_prefix}')\")\n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"AVISO: Diretório de origem não encontrado: {source_dir}. Pulando.\")\n",
        "        continue\n",
        "\n",
        "    source_files = os.listdir(source_dir)\n",
        "    total_files_in_sources += len(source_files)\n",
        "\n",
        "    for filename in tqdm(source_files, desc=f\"Processando {camera_prefix}\"):\n",
        "        if not filename.lower().endswith(('.jpg', '.jpeg')):\n",
        "            counters['skipped_other'] += 1\n",
        "            continue\n",
        "\n",
        "        source_path = os.path.join(source_dir, filename)\n",
        "        match = filename_pattern.match(filename)\n",
        "\n",
        "        if not match:\n",
        "            counters['skipped_pattern'] += 1\n",
        "            continue\n",
        "\n",
        "        sample_num = int(match.group(1))\n",
        "\n",
        "        # 1. FILTRO DE EXCLUSÃO\n",
        "        if sample_num in EXCLUDE_BOX_NUMBERS:\n",
        "            counters['skipped_excluded'] += 1\n",
        "            continue\n",
        "\n",
        "        # --- Correção da ordem do nome do arquivo para Câmera 2 ---\n",
        "        corrected_filename = filename\n",
        "        if camera_prefix == 'cam2':\n",
        "            match_fix = re.match(r'(.*)\\.(b2?)\\.(\\d+)\\.jpg', filename, re.IGNORECASE)\n",
        "            if match_fix:\n",
        "                base, esteira_id, num = match_fix.groups()\n",
        "                corrected_filename = f\"{base}.{num}.{esteira_id}.jpg\"\n",
        "\n",
        "        # 2. FILTRO DE LOCALIZAÇÃO (usa o nome corrigido para consistência)\n",
        "        location = get_location(corrected_filename)\n",
        "        if location not in INCLUDE_LOCATIONS:\n",
        "            counters['skipped_location'] += 1\n",
        "            continue\n",
        "\n",
        "        # --- Renomeia o arquivo (troca '.' por '_') e adiciona prefixo ---\n",
        "        base_name, extension = os.path.splitext(corrected_filename)\n",
        "        new_base_name = base_name.replace('.', '_')\n",
        "        new_filename_with_prefix = f\"{camera_prefix}_{new_base_name}{extension}\"\n",
        "\n",
        "        # 3. CLASSIFICAÇÃO (GOOD vs BAD)\n",
        "        is_good_class = sample_num in GOOD_BOX_NUMBERS\n",
        "\n",
        "        # 4. DIVISÃO (TRAIN vs TEST)\n",
        "        # --- NOVA ALTERAÇÃO: Verifica o nome COM prefixo na lista de treino ---\n",
        "        is_for_training = (sample_num in TRAIN_BOX_NUMBERS) or (new_filename_with_prefix in TRAIN_SPECIFIC_FILENAMES)\n",
        "\n",
        "        dest_sub_dir = None\n",
        "        if is_for_training:\n",
        "            if is_good_class:\n",
        "                dest_sub_dir = train_good_dir\n",
        "                counters['train_good'] += 1\n",
        "            else:\n",
        "                print(f\"AVISO: Tentativa de adicionar amostra 'bad' ({filename}) ao conjunto de treino. Pulando.\")\n",
        "                counters['warn_bad_in_train'] += 1\n",
        "                continue\n",
        "        else:\n",
        "            if is_good_class:\n",
        "                dest_sub_dir = test_good_dir\n",
        "                counters['test_good'] += 1\n",
        "            else:\n",
        "                dest_sub_dir = test_bad_dir\n",
        "                counters['test_bad'] += 1\n",
        "\n",
        "        # Copia o arquivo para o destino correto\n",
        "        dest_path = os.path.join(dest_sub_dir, new_filename_with_prefix)\n",
        "        try:\n",
        "            shutil.copy2(source_path, dest_path)\n",
        "        except Exception as e:\n",
        "            print(f\"ERRO ao copiar {filename}: {e}\")\n",
        "            if dest_sub_dir == train_good_dir: counters['train_good'] -= 1\n",
        "            elif dest_sub_dir == test_good_dir: counters['test_good'] -= 1\n",
        "            elif dest_sub_dir == test_bad_dir: counters['test_bad'] -= 1\n",
        "            counters['skipped_other'] += 1\n",
        "\n",
        "# --- Relatório Final ---\n",
        "total_copied = counters['train_good'] + counters['test_good'] + counters['test_bad']\n",
        "total_skipped = total_files_in_sources - total_copied - counters['warn_bad_in_train']\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Organização Concluída!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\n--- Resumo da Geração do Dataset ---\")\n",
        "print(f\"Imagens copiadas para 'train/good': {counters['train_good']}\")\n",
        "print(f\"Imagens copiadas para 'test/good':  {counters['test_good']}\")\n",
        "print(f\"Imagens copiadas para 'test/bad':   {counters['test_bad']}\")\n",
        "print(f\"-------------------------------------------\")\n",
        "print(f\"Total de imagens no novo dataset: {total_copied}\")\n",
        "\n",
        "print(\"\\n--- Resumo das Imagens Ignoradas ---\")\n",
        "print(f\"Puladas por estarem na lista de exclusão: {counters['skipped_excluded']}\")\n",
        "print(f\"Puladas por filtro de localização:        {counters['skipped_location']}\")\n",
        "print(f\"Puladas por não corresponder ao padrão:   {counters['skipped_pattern']}\")\n",
        "print(f\"Avisos de amostras 'bad' em treino:       {counters['warn_bad_in_train']}\")\n",
        "print(f\"Outros arquivos ignorados (não-jpg):    {counters['skipped_other']}\")\n",
        "print(f\"-------------------------------------------\")\n",
        "print(f\"Total de arquivos ignorados: {total_skipped + counters['warn_bad_in_train']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Total de arquivos nos diretórios de origem: {total_files_in_sources}\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0XOLQcyGd9q",
        "outputId": "304ad7b0-3fde-4c58-b106-6c1b1817459a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Montando Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Limpando diretório de destino antigo: /content/UniVAD/data/CardboardBox/cardboard_box\n",
            "Criando nova estrutura de pastas de destino...\n",
            "\n",
            "Iniciando a organização do dataset...\n",
            "\n",
            "Processando diretório: /content/drive/MyDrive/TCC_Dataset/Câmera1 (prefixo: 'cam1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processando cam1: 100%|██████████| 282/282 [00:07<00:00, 35.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processando diretório: /content/drive/MyDrive/TCC_Dataset/Câmera2 (prefixo: 'cam2')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processando cam2: 100%|██████████| 274/274 [00:07<00:00, 36.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Organização Concluída!\n",
            "==================================================\n",
            "\n",
            "--- Resumo da Geração do Dataset ---\n",
            "Imagens copiadas para 'train/good': 1\n",
            "Imagens copiadas para 'test/good':  152\n",
            "Imagens copiadas para 'test/bad':   356\n",
            "-------------------------------------------\n",
            "Total de imagens no novo dataset: 509\n",
            "\n",
            "--- Resumo das Imagens Ignoradas ---\n",
            "Puladas por estarem na lista de exclusão: 44\n",
            "Puladas por filtro de localização:        0\n",
            "Puladas por não corresponder ao padrão:   0\n",
            "Avisos de amostras 'bad' em treino:       0\n",
            "Outros arquivos ignorados (não-jpg):    3\n",
            "-------------------------------------------\n",
            "Total de arquivos ignorados: 47\n",
            "\n",
            "==================================================\n",
            "Total de arquivos nos diretórios de origem: 556\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Baixando o dataset VisA e a ferramenta de preparação...\")\n",
        "\n",
        "# # Baixa o dataset para o diretório atual (/content/UniVAD)\n",
        "# !wget -O VisA.tar \"https://amazon-visual-anomaly.s3.us-west-2.amazonaws.com/VisA_20220922.tar\"\n",
        "\n",
        "# # Cria a pasta de dados brutos FORA da pasta do projeto e extrai os arquivos para lá\n",
        "# !mkdir -p /content/visa_raw\n",
        "# !tar -xf VisA.tar -C /content/visa_raw\n",
        "# print(\"Dataset extraído em /content/visa_raw\")\n",
        "\n",
        "# # Clona a ferramenta de preparação FORA da pasta do projeto\n",
        "# !git clone https://github.com/amazon-science/spot-diff.git /content/spot-diff-tool\n",
        "# print(\"Ferramenta de preparação clonada em /content/spot-diff-tool\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFjKNslD_rQO",
        "outputId": "59498a60-305c-429f-8a0b-95b5caa06d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baixando o dataset VisA e a ferramenta de preparação...\n",
            "--2025-10-29 12:49:10--  https://amazon-visual-anomaly.s3.us-west-2.amazonaws.com/VisA_20220922.tar\n",
            "Resolving amazon-visual-anomaly.s3.us-west-2.amazonaws.com (amazon-visual-anomaly.s3.us-west-2.amazonaws.com)... 3.5.77.104, 52.92.195.138, 52.92.187.82, ...\n",
            "Connecting to amazon-visual-anomaly.s3.us-west-2.amazonaws.com (amazon-visual-anomaly.s3.us-west-2.amazonaws.com)|3.5.77.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1929840640 (1.8G) [application/x-tar]\n",
            "Saving to: ‘VisA.tar’\n",
            "\n",
            "VisA.tar            100%[===================>]   1.80G  58.4MB/s    in 27s     \n",
            "\n",
            "2025-10-29 12:49:37 (68.5 MB/s) - ‘VisA.tar’ saved [1929840640/1929840640]\n",
            "\n",
            "Dataset extraído em /content/visa_raw\n",
            "Cloning into '/content/spot-diff-tool'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 65 (delta 31), reused 47 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (65/65), 2.42 MiB | 7.52 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n",
            "Ferramenta de preparação clonada em /content/spot-diff-tool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"\\nReorganizando o dataset para o formato do UniVAD...\")\n",
        "# !python /content/spot-diff-tool/utils/prepare_data.py \\\n",
        "#     --split-type 1cls \\\n",
        "#     --data-folder /content/visa_raw \\\n",
        "#     --save-folder /content/UniVAD/data/VisA_pytorch \\\n",
        "#     --split-file /content/spot-diff-tool/split_csv/1cls.csv\n",
        "# print(\"Dataset reorganizado com sucesso em ./data/VisA_pytorch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHhqKm-q_wQf",
        "outputId": "3126c3a5-5ac6-40c7-e3fd-34dcc8982397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reorganizando o dataset para o formato do UniVAD...\n",
            "Dataset reorganizado com sucesso em ./data/VisA_pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python test_univad.py --dataset visa --data_path ./data/VisA_pytorch/1cls --round 0 --image_size 224 --k_shot 1 --class_name chewinggum"
      ],
      "metadata": {
        "id": "8la7TivP_zy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_univad.py --dataset cardboard_box --data_path ./data/CardboardBox --k_shot 1 --round 0 --image_size 224 --filter_with_mask --anomaly_threshold 0.68 --class_name cardboard_box --debug"
      ],
      "metadata": {
        "id": "nwWR0eA-JYYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b19461b-206b-4040-907d-557b0491a17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-04 13:20:21.235865: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762262421.272486   10707 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762262421.284435   10707 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762262421.321440   10707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762262421.321490   10707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762262421.321497   10707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762262421.321504   10707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-04 13:20:21.328433: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "13:20:30 - image_size: 224\n",
            "13:20:30 - k_shot: 1\n",
            "13:20:30 - dataset: cardboard_box\n",
            "13:20:30 - data_path: ./data/CardboardBox\n",
            "13:20:30 - save_path: ./results/\n",
            "13:20:30 - round: 0\n",
            "13:20:30 - class_name: cardboard_box\n",
            "13:20:30 - device: cuda\n",
            "13:20:30 - config_path_base: ./configs/class_histogram\n",
            "13:20:30 - anomaly_threshold: 0.68\n",
            "13:20:30 - seed: 42\n",
            "13:20:30 - filter_with_mask: True\n",
            "13:20:30 - debug: True\n",
            "13:20:30 - top_k_percent: 0.0\n",
            "13:20:30 - Seed para reprodutibilidade definida: 42\n",
            "13:20:30 - Carregando modelo UniVAD...\n",
            "Carregando DINOv2-G...\n",
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "13:20:30 - using SwiGLU layer as FFN\n",
            "DINOv2-G carregado e movido para a GPU.\n",
            "Carregando CLIP-L (via internet)...\n",
            "13:21:11 - Loading pretrained ViT-L-14-336 from OpenAI.\n",
            "13:21:27 - Resizing position embedding grid-size from (24, 24) to (16, 16)\n",
            "CLIP-L carregado e movido para a GPU.\n",
            "13:21:33 - Modelo UniVAD carregado.\n",
            "13:21:33 - Carregando modelos de segmentação (GroundingDINO + SAM)...\n",
            "SegmentationHandler inicializado. Chame `load_models()` para carregar os pesos.\n",
            "Carregando modelo GroundingDINO na VRAM...\n",
            "final text_encoder_type: bert-base-uncased\n",
            "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
            "Carregando modelo Segment Anything (SAM) na VRAM...\n",
            "_IncompatibleKeys(missing_keys=['mask_decoder.hf_token.weight', 'mask_decoder.hf_mlp.layers.0.weight', 'mask_decoder.hf_mlp.layers.0.bias', 'mask_decoder.hf_mlp.layers.1.weight', 'mask_decoder.hf_mlp.layers.1.bias', 'mask_decoder.hf_mlp.layers.2.weight', 'mask_decoder.hf_mlp.layers.2.bias', 'mask_decoder.compress_vit_feat.0.weight', 'mask_decoder.compress_vit_feat.0.bias', 'mask_decoder.compress_vit_feat.1.weight', 'mask_decoder.compress_vit_feat.1.bias', 'mask_decoder.compress_vit_feat.3.weight', 'mask_decoder.compress_vit_feat.3.bias', 'mask_decoder.embedding_encoder.0.weight', 'mask_decoder.embedding_encoder.0.bias', 'mask_decoder.embedding_encoder.1.weight', 'mask_decoder.embedding_encoder.1.bias', 'mask_decoder.embedding_encoder.3.weight', 'mask_decoder.embedding_encoder.3.bias', 'mask_decoder.embedding_maskfeature.0.weight', 'mask_decoder.embedding_maskfeature.0.bias', 'mask_decoder.embedding_maskfeature.1.weight', 'mask_decoder.embedding_maskfeature.1.bias', 'mask_decoder.embedding_maskfeature.3.weight', 'mask_decoder.embedding_maskfeature.3.bias'], unexpected_keys=[])\n",
            "Modelos de segmentação prontos para uso.\n",
            "13:21:42 - DataLoader shuffle está ATIVADO\n",
            "13:21:42 - Iniciando o loop de inferência sequencial...\n",
            "Processando Amostras:   0% 0/257 [00:00<?, ?it/s]13:21:43 - \n",
            "Configurando para a nova classe: cardboard box\n",
            "13:21:43 - Config de segmentação para cardboard box carregada.\n",
            "13:21:43 - Gerando máscaras de referência (k-shot) em: ./masks/CardboardBox/cardboard_box\n",
            "\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:04<00:00,  4.96s/it]\n",
            "13:21:48 - Aplicando workaround de renomeação nas máscaras de referência...\n",
            "13:21:49 - Setup do UniVAD para cardboard box concluído.\n",
            "\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:02<00:00,  2.48s/it]\n",
            "13:21:52 - ❌ Imagem: board41_2.jpg        | Score: 0.5960 (Score Filtrado Top-Max: 0.5960) | Prev: GOOD | Real: BAD \n",
            "Processando Amostras:   0% 1/257 [00:10<43:00, 10.08s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.63s/it]\n",
            "13:21:54 - ❌ Imagem: board16_4_1.jpg      | Score: 0.7905 (Score Filtrado Top-Max: 0.7905) | Prev: BAD  | Real: GOOD\n",
            "Processando Amostras:   1% 2/257 [00:12<22:56,  5.40s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:02<00:00,  2.67s/it]\n",
            "13:21:57 - ✅ Imagem: board30_1_b.jpg      | Score: 1.5966 (Score Filtrado Top-Max: 1.5966) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   1% 3/257 [00:15<18:33,  4.38s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.31s/it]\n",
            "13:21:59 - ✅ Imagem: board37_1.jpg        | Score: 0.7685 (Score Filtrado Top-Max: 0.7685) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   2% 4/257 [00:17<14:10,  3.36s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.32s/it]\n",
            "13:22:01 - ✅ Imagem: board2_1_2.jpg       | Score: 0.7066 (Score Filtrado Top-Max: 0.7066) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   2% 5/257 [00:18<11:46,  2.80s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.44s/it]\n",
            "13:22:03 - ✅ Imagem: board32_1_b2.jpg     | Score: 0.5477 (Score Filtrado Top-Max: 0.5477) | Prev: GOOD | Real: GOOD\n",
            "Processando Amostras:   2% 6/257 [00:20<10:33,  2.53s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.67s/it]\n",
            "13:22:05 - ❌ Imagem: board8_4.jpg         | Score: 0.6474 (Score Filtrado Top-Max: 0.6474) | Prev: GOOD | Real: BAD \n",
            "Processando Amostras:   3% 7/257 [00:23<10:07,  2.43s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.32s/it]\n",
            "13:22:07 - ✅ Imagem: board2_2_2.jpg       | Score: 0.7537 (Score Filtrado Top-Max: 0.7537) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   3% 8/257 [00:25<09:16,  2.23s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.92s/it]\n",
            "13:22:09 - ✅ Imagem: board29_2_b.jpg      | Score: 1.7079 (Score Filtrado Top-Max: 1.7079) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   4% 9/257 [00:27<09:29,  2.30s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.71s/it]\n",
            "13:22:11 - ✅ Imagem: board13_2.jpg        | Score: 0.7400 (Score Filtrado Top-Max: 0.7400) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   4% 10/257 [00:29<09:20,  2.27s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.30s/it]\n",
            "13:22:13 - ❌ Imagem: board39_4.jpg        | Score: 0.6794 (Score Filtrado Top-Max: 0.6794) | Prev: GOOD | Real: BAD \n",
            "Processando Amostras:   4% 11/257 [00:31<08:42,  2.12s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.61s/it]\n",
            "13:22:15 - ❌ Imagem: board43_2.jpg        | Score: 0.6192 (Score Filtrado Top-Max: 0.6192) | Prev: GOOD | Real: BAD \n",
            "Processando Amostras:   5% 12/257 [00:33<08:43,  2.14s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:02<00:00,  2.17s/it]\n",
            "13:22:18 - ✅ Imagem: board35_4_1.jpg      | Score: 0.8770 (Score Filtrado Top-Max: 0.8770) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   5% 13/257 [00:36<09:25,  2.32s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.32s/it]\n",
            "13:22:20 - ✅ Imagem: board15_1_b.jpg      | Score: 0.6490 (Score Filtrado Top-Max: 0.6490) | Prev: GOOD | Real: GOOD\n",
            "Processando Amostras:   5% 14/257 [00:38<08:45,  2.16s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.31s/it]\n",
            "13:22:22 - ✅ Imagem: board18_4.jpg        | Score: 0.6703 (Score Filtrado Top-Max: 0.6703) | Prev: GOOD | Real: GOOD\n",
            "Processando Amostras:   6% 15/257 [00:39<08:17,  2.06s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.33s/it]\n",
            "13:22:23 - ✅ Imagem: board14_3_1.jpg      | Score: 0.5950 (Score Filtrado Top-Max: 0.5950) | Prev: GOOD | Real: GOOD\n",
            "Processando Amostras:   6% 16/257 [00:41<07:59,  1.99s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.31s/it]\n",
            "13:22:25 - ❌ Imagem: board3_3_b.jpg       | Score: 0.6791 (Score Filtrado Top-Max: 0.6791) | Prev: GOOD | Real: BAD \n",
            "Processando Amostras:   7% 17/257 [00:43<07:44,  1.94s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.68s/it]\n",
            "13:22:27 - ✅ Imagem: board7_1_b.jpg       | Score: 1.0562 (Score Filtrado Top-Max: 1.0562) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   7% 18/257 [00:45<08:01,  2.02s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:02<00:00,  2.44s/it]\n",
            "13:22:30 - ✅ Imagem: board12_3.jpg        | Score: 0.9899 (Score Filtrado Top-Max: 0.9899) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   7% 19/257 [00:48<09:10,  2.31s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:01<00:00,  1.80s/it]\n",
            "13:22:33 - ✅ Imagem: board5_2.jpg         | Score: 0.7628 (Score Filtrado Top-Max: 0.7628) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   8% 20/257 [00:51<09:08,  2.31s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Gerando Máscaras (rápido): 100% 1/1 [00:02<00:00,  2.13s/it]\n",
            "13:22:35 - ✅ Imagem: board28_1_b.jpg      | Score: 1.7797 (Score Filtrado Top-Max: 1.7797) | Prev: BAD  | Real: BAD \n",
            "Processando Amostras:   8% 21/257 [00:53<09:30,  2.42s/it]\n",
            "Gerando Máscaras (rápido):   0% 0/1 [00:00<?, ?it/s]\n",
            "Processando Amostras:   8% 21/257 [00:54<10:08,  2.58s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/UniVAD/test_univad.py\", line 199, in <module>\n",
            "    segmentation_handler.segment([image_path_linux], mask_output_dir, grounding_config)\n",
            "  File \"/content/UniVAD/models/component_segmentaion.py\", line 112, in segment\n",
            "    image_pil, image_tensor = load_image(image_path)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/UniVAD/models/grounded_sam.py\", line 40, in load_image\n",
            "    image, _ = transform(image_pil, None)  # 3, h, w\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/UniVAD/models/GroundingDINO/groundingdino/datasets/transforms.py\", line 302, in __call__\n",
            "    image, target = t(image, target)\n",
            "                    ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/UniVAD/models/GroundingDINO/groundingdino/datasets/transforms.py\", line 234, in __call__\n",
            "    return resize(img, target, size, self.max_size)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/UniVAD/models/GroundingDINO/groundingdino/datasets/transforms.py\", line 117, in resize\n",
            "    rescaled_image = F.resize(image, size)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\", line 467, in resize\n",
            "    return F_pil.resize(img, size=output_size, interpolation=pil_interpolation)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_functional_pil.py\", line 250, in resize\n",
            "    return img.resize(tuple(size[::-1]), interpolation)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/Image.py\", line 2328, in resize\n",
            "    return self._new(self.im.resize(size, resample, box))\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python test_univad.py --dataset cardboard_box --data_path ./data/CardboardBox --k_shot 1 --round 0 --image_size 224 --filter_with_mask --anomaly_threshold 0.68 --top_k_percent 0.01 --use_all_clip_layers --class_name cardboard_box --debug"
      ],
      "metadata": {
        "id": "UflJ348DMJmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar um .zip da pasta de resultados\n",
        "!zip -r /content/results.zip /content/UniVAD/results"
      ],
      "metadata": {
        "id": "nsTCDAUAJrLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar um .zip da pasta de máscaras\n",
        "!zip -r /content/results.zip /content/UniVAD/masks"
      ],
      "metadata": {
        "id": "5ofHUPMdKJdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1JE-9Gt8OhQ"
      },
      "outputs": [],
      "source": [
        "# Limpar as pastas de testes anteriores\n",
        "\n",
        "! rm -rf /content/UniVAD/results\n",
        "! rm -rf /content/UniVAD/masks\n",
        "! rm -rf /content/UniVAD/heat_masks"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
